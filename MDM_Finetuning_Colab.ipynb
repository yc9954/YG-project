{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "header"
   },
   "source": [
    "# ğŸ¬ K-Pop ì•ˆë¬´ AI íŒŒì¸íŠœë‹ - Google Colab\n",
    "\n",
    "## ğŸ“Œ ì‚¬ìš© ë°©ë²•\n",
    "1. **GPU í™œì„±í™”**: `Runtime > Change runtime type > GPU (T4)`\n",
    "2. **ìˆœì„œëŒ€ë¡œ ì‹¤í–‰**: ê° ì…€ì„ ìœ„ì—ì„œ ì•„ë˜ë¡œ ì‹¤í–‰\n",
    "3. **ì˜ìƒ ì—…ë¡œë“œ**: K-pop ëŒ„ìŠ¤ ì˜ìƒ ì—…ë¡œë“œ\n",
    "4. **ìë™ íŒŒì¸íŠœë‹**: ë‚˜ë¨¸ì§€ëŠ” ìë™ìœ¼ë¡œ ì§„í–‰!\n",
    "\n",
    "## âœ¨ ì£¼ìš” ê¸°ëŠ¥\n",
    "- âœ… K-pop ì˜ìƒ ìë™ ë¶„ì„\n",
    "- âœ… í¬ì¦ˆ ì¶”ì¶œ ë° ë³€í™˜\n",
    "- âœ… ìë™ íŒŒì¸íŠœë‹\n",
    "- âœ… Google Drive ì²´í¬í¬ì¸íŠ¸ ì €ì¥\n",
    "- âœ… ì‹¤ì‹œê°„ í•™ìŠµ ëª¨ë‹ˆí„°ë§"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "section_1"
   },
   "source": [
    "---\n",
    "## 1ï¸âƒ£ í™˜ê²½ ì„¤ì • ë° íŒ¨í‚¤ì§€ ì„¤ì¹˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "check_gpu"
   },
   "outputs": [],
   "source": [
    "# GPU í™•ì¸\n",
    "import torch\n",
    "print(\"=\" * 60)\n",
    "print(\"ğŸ” ì‹œìŠ¤í…œ ì •ë³´\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"PyTorch ë²„ì „: {torch.__version__}\")\n",
    "print(f\"CUDA ì‚¬ìš© ê°€ëŠ¥: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"CUDA ë²„ì „: {torch.version.cuda}\")\n",
    "    print(f\"GPU ë©”ëª¨ë¦¬: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\n",
    "else:\n",
    "    print(\"âš ï¸  GPUë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. Runtime > Change runtime typeì—ì„œ GPUë¥¼ í™œì„±í™”í•˜ì„¸ìš”.\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "install_packages"
   },
   "outputs": [],
   "source": [
    "# í•„ìˆ˜ íŒ¨í‚¤ì§€ ì„¤ì¹˜\n",
    "print(\"ğŸ“¦ íŒ¨í‚¤ì§€ ì„¤ì¹˜ ì¤‘... (3-5ë¶„ ì†Œìš”)\")\n",
    "\n",
    "!pip install -q torch torchvision torchaudio\n",
    "!pip install -q opencv-python-headless\n",
    "!pip install -q mediapipe\n",
    "!pip install -q numpy scipy scikit-learn\n",
    "!pip install -q matplotlib seaborn tqdm\n",
    "!pip install -q ipywidgets\n",
    "\n",
    "print(\"âœ… íŒ¨í‚¤ì§€ ì„¤ì¹˜ ì™„ë£Œ!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mount_drive"
   },
   "outputs": [],
   "source": [
    "# Google Drive ë§ˆìš´íŠ¸\n",
    "from google.colab import drive\n",
    "import os\n",
    "\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# í”„ë¡œì íŠ¸ ë””ë ‰í† ë¦¬ ìƒì„±\n",
    "PROJECT_DIR = '/content/drive/MyDrive/KPop_AI_Studio'\n",
    "CHECKPOINT_DIR = f'{PROJECT_DIR}/checkpoints'\n",
    "TRAINING_DATA_DIR = f'{PROJECT_DIR}/training_data'\n",
    "\n",
    "os.makedirs(PROJECT_DIR, exist_ok=True)\n",
    "os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
    "os.makedirs(TRAINING_DATA_DIR, exist_ok=True)\n",
    "\n",
    "print(\"âœ… Google Drive ì—°ë™ ì™„ë£Œ!\")\n",
    "print(f\"ğŸ“ í”„ë¡œì íŠ¸ ë””ë ‰í† ë¦¬: {PROJECT_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "section_2"
   },
   "source": [
    "---\n",
    "## 2ï¸âƒ£ K-Pop ì˜ìƒ ì—…ë¡œë“œ ë° ì „ì²˜ë¦¬"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "upload_videos"
   },
   "outputs": [],
   "source": [
    "# ì˜ìƒ ì—…ë¡œë“œ\n",
    "from google.colab import files\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "\n",
    "print(\"ğŸ¬ K-Pop ëŒ„ìŠ¤ ì˜ìƒì„ ì—…ë¡œë“œí•˜ì„¸ìš”!\")\n",
    "print(\"   ê¶Œì¥: 10-30ì´ˆ ê¸¸ì´, 1-5ê°œ ì˜ìƒ\")\n",
    "print(\"   í˜•ì‹: MP4, MOV, AVI ë“±\")\n",
    "print(\"\")\n",
    "\n",
    "uploaded = files.upload()\n",
    "\n",
    "# ì—…ë¡œë“œëœ íŒŒì¼ ì €ì¥\n",
    "VIDEO_DIR = '/content/videos'\n",
    "os.makedirs(VIDEO_DIR, exist_ok=True)\n",
    "\n",
    "video_files = []\n",
    "for filename in uploaded.keys():\n",
    "    video_path = f\"{VIDEO_DIR}/{filename}\"\n",
    "    with open(video_path, 'wb') as f:\n",
    "        f.write(uploaded[filename])\n",
    "    video_files.append(video_path)\n",
    "    print(f\"âœ… ì—…ë¡œë“œ ì™„ë£Œ: {filename}\")\n",
    "\n",
    "print(f\"\\nì´ {len(video_files)}ê°œ ì˜ìƒ ì—…ë¡œë“œ ì™„ë£Œ!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "input_captions"
   },
   "outputs": [],
   "source": [
    "# í…ìŠ¤íŠ¸ ìº¡ì…˜ ì…ë ¥\n",
    "from IPython.display import display, HTML\n",
    "import ipywidgets as widgets\n",
    "\n",
    "print(\"ğŸ“ ê° ì˜ìƒì— ëŒ€í•œ ì„¤ëª…ì„ ì…ë ¥í•˜ì„¸ìš”\")\n",
    "print(\"   ì˜ˆì‹œ: 'Powerful hip-hop choreography with sharp movements'\")\n",
    "print(\"   ì˜ˆì‹œ: 'Smooth girl group dance with elegant arm waves'\")\n",
    "print(\"\")\n",
    "\n",
    "captions = {}\n",
    "caption_widgets = {}\n",
    "\n",
    "for video_path in video_files:\n",
    "    video_name = Path(video_path).name\n",
    "    text_widget = widgets.Text(\n",
    "        value='',\n",
    "        placeholder=f'{video_name}ì— ëŒ€í•œ ì„¤ëª… ì…ë ¥...',\n",
    "        description=f'{video_name[:20]}...',\n",
    "        style={'description_width': 'initial'},\n",
    "        layout=widgets.Layout(width='80%')\n",
    "    )\n",
    "    caption_widgets[video_path] = text_widget\n",
    "    display(text_widget)\n",
    "\n",
    "print(\"\\nâ¬†ï¸ ìœ„ì— ê° ì˜ìƒì˜ ì„¤ëª…ì„ ì…ë ¥í•œ í›„ ë‹¤ìŒ ì…€ì„ ì‹¤í–‰í•˜ì„¸ìš”!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "save_captions"
   },
   "outputs": [],
   "source": [
    "# ìº¡ì…˜ ì €ì¥\n",
    "for video_path, widget in caption_widgets.items():\n",
    "    caption = widget.value.strip()\n",
    "    if not caption:\n",
    "        caption = f\"Dance performance from {Path(video_path).stem}\"\n",
    "    captions[video_path] = caption\n",
    "    print(f\"âœ… {Path(video_path).name}: {caption}\")\n",
    "\n",
    "print(f\"\\nì´ {len(captions)}ê°œ ìº¡ì…˜ ì €ì¥ ì™„ë£Œ!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "extract_motion"
   },
   "outputs": [],
   "source": [
    "# ëª¨ì…˜ ì¶”ì¶œ ë° ë³€í™˜\n",
    "import cv2\n",
    "import numpy as np\n",
    "import mediapipe as mp\n",
    "from tqdm.notebook import tqdm\n",
    "import json\n",
    "\n",
    "print(\"ğŸ¯ ì˜ìƒì—ì„œ ëª¨ì…˜ ì¶”ì¶œ ì¤‘...\\n\")\n",
    "\n",
    "# MediaPipe Pose ì´ˆê¸°í™”\n",
    "mp_pose = mp.solutions.pose\n",
    "pose = mp_pose.Pose(\n",
    "    static_image_mode=False,\n",
    "    model_complexity=2,\n",
    "    smooth_landmarks=True,\n",
    "    min_detection_confidence=0.5,\n",
    "    min_tracking_confidence=0.5\n",
    ")\n",
    "\n",
    "def extract_motion_from_video(video_path, fps=20):\n",
    "    \"\"\"ë¹„ë””ì˜¤ì—ì„œ ëª¨ì…˜ ì¶”ì¶œ\"\"\"\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    video_fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    \n",
    "    frame_interval = max(1, int(video_fps / fps))\n",
    "    motion_data = []\n",
    "    frame_count = 0\n",
    "    \n",
    "    pbar = tqdm(total=total_frames, desc=f\"  {Path(video_path).name}\")\n",
    "    \n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        \n",
    "        if frame_count % frame_interval == 0:\n",
    "            frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            results = pose.process(frame_rgb)\n",
    "            \n",
    "            if results.pose_landmarks:\n",
    "                landmarks = np.array([[lm.x * width, lm.y * height, lm.z * width] \n",
    "                                     for lm in results.pose_landmarks.landmark])\n",
    "                motion_data.append(landmarks)\n",
    "            elif motion_data:\n",
    "                motion_data.append(motion_data[-1])\n",
    "            else:\n",
    "                motion_data.append(np.zeros((33, 3)))\n",
    "        \n",
    "        frame_count += 1\n",
    "        pbar.update(1)\n",
    "    \n",
    "    cap.release()\n",
    "    pbar.close()\n",
    "    \n",
    "    return np.array(motion_data)\n",
    "\n",
    "def convert_to_humanml3d(motion_data):\n",
    "    \"\"\"MediaPipeë¥¼ HumanML3D í˜•ì‹ìœ¼ë¡œ ë³€í™˜\"\"\"\n",
    "    # ê°„ë‹¨í•œ ë§¤í•‘ (33 -> 22 joints)\n",
    "    mapping = {\n",
    "        0: [23, 24], 1: [11, 12], 2: [0], 3: [0],  # ì¤‘ì‹¬\n",
    "        4: [23], 5: [25], 6: [27], 7: [31],  # ì™¼ìª½ ë‹¤ë¦¬\n",
    "        8: [24], 9: [26], 10: [28], 11: [32],  # ì˜¤ë¥¸ìª½ ë‹¤ë¦¬\n",
    "        12: [11], 13: [13], 14: [15], 15: [19],  # ì™¼ìª½ íŒ”\n",
    "        16: [12], 17: [14], 18: [16], 19: [20],  # ì˜¤ë¥¸ìª½ íŒ”\n",
    "        20: [0], 21: [0]  # ë¨¸ë¦¬\n",
    "    }\n",
    "    \n",
    "    frames = motion_data.shape[0]\n",
    "    humanml_data = np.zeros((frames, 22, 3))\n",
    "    \n",
    "    for frame_idx in range(frames):\n",
    "        for smpl_idx, mp_indices in mapping.items():\n",
    "            positions = [motion_data[frame_idx, idx] for idx in mp_indices]\n",
    "            humanml_data[frame_idx, smpl_idx] = np.mean(positions, axis=0)\n",
    "    \n",
    "    # ì •ê·œí™”\n",
    "    pelvis = humanml_data[:, 0:1, :]\n",
    "    humanml_data = humanml_data - pelvis\n",
    "    \n",
    "    return humanml_data\n",
    "\n",
    "# ëª¨ë“  ì˜ìƒ ì²˜ë¦¬\n",
    "dataset = []\n",
    "for idx, video_path in enumerate(video_files):\n",
    "    print(f\"\\n[{idx+1}/{len(video_files)}] ì²˜ë¦¬ ì¤‘: {Path(video_path).name}\")\n",
    "    \n",
    "    # ëª¨ì…˜ ì¶”ì¶œ\n",
    "    motion_data = extract_motion_from_video(video_path, fps=20)\n",
    "    print(f\"  âœ“ ì¶”ì¶œ ì™„ë£Œ: {motion_data.shape[0]}í”„ë ˆì„\")\n",
    "    \n",
    "    # HumanML3D ë³€í™˜\n",
    "    humanml_data = convert_to_humanml3d(motion_data)\n",
    "    print(f\"  âœ“ ë³€í™˜ ì™„ë£Œ: {humanml_data.shape}\")\n",
    "    \n",
    "    # ì €ì¥\n",
    "    sample_name = f\"sample_{idx:04d}\"\n",
    "    motion_path = f\"{TRAINING_DATA_DIR}/{sample_name}.npy\"\n",
    "    np.save(motion_path, humanml_data)\n",
    "    \n",
    "    dataset.append({\n",
    "        'id': idx,\n",
    "        'name': sample_name,\n",
    "        'caption': captions[video_path],\n",
    "        'video_name': Path(video_path).name,\n",
    "        'frames': humanml_data.shape[0],\n",
    "        'motion_path': motion_path\n",
    "    })\n",
    "    print(f\"  âœ“ ì €ì¥ ì™„ë£Œ: {motion_path}\")\n",
    "\n",
    "# ë°ì´í„°ì…‹ ë©”íƒ€ë°ì´í„° ì €ì¥\n",
    "dataset_path = f\"{TRAINING_DATA_DIR}/dataset.json\"\n",
    "with open(dataset_path, 'w', encoding='utf-8') as f:\n",
    "    json.dump(dataset, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"âœ… ì „ì²˜ë¦¬ ì™„ë£Œ!\")\n",
    "print(f\"ğŸ“Š ì´ {len(dataset)}ê°œ ìƒ˜í”Œ ìƒì„±\")\n",
    "print(f\"ğŸ’¾ ë°ì´í„°ì…‹ ì €ì¥: {dataset_path}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "section_3"
   },
   "source": [
    "---\n",
    "## 3ï¸âƒ£ íŒŒì¸íŠœë‹ ì„¤ì • ë° ì‹œì‘"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "training_setup"
   },
   "outputs": [],
   "source": "# í•™ìŠµ ì„¤ì •\nfrom dataclasses import dataclass\n\n@dataclass\nclass TrainingConfig:\n    learning_rate: float = 1e-4\n    batch_size: int = 4\n    num_epochs: int = 50\n    save_interval: int = 500  # 500 ìŠ¤í…ë§ˆë‹¤ ì €ì¥\n    log_interval: int = 50    # 50 ìŠ¤í…ë§ˆë‹¤ ë¡œê·¸\n    max_motion_length: int = 6000  # 5ë¶„ ì˜ìƒ (300ì´ˆ Ã— 20fps)\n    diffusion_steps: int = 1000\n\nconfig = TrainingConfig()\n\nprint(\"âš™ï¸ í•™ìŠµ ì„¤ì •\")\nprint(\"=\"*60)\nprint(f\"Learning Rate: {config.learning_rate}\")\nprint(f\"Batch Size: {config.batch_size}\")\nprint(f\"Epochs: {config.num_epochs}\")\nprint(f\"Save Interval: {config.save_interval} steps\")\nprint(f\"Max Motion Length: {config.max_motion_length} frames (~{config.max_motion_length/20:.0f}ì´ˆ)\")\nprint(f\"Training Samples: {len(dataset)}\")\nprint(\"=\"*60)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "create_dataloader"
   },
   "outputs": [],
   "source": "# ë°ì´í„°ë¡œë” ìƒì„±\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\n\nclass MotionDataset(Dataset):\n    def __init__(self, dataset_info, max_length=6000):\n        self.samples = dataset_info\n        self.max_length = max_length\n    \n    def __len__(self):\n        return len(self.samples)\n    \n    def __getitem__(self, idx):\n        sample = self.samples[idx]\n        motion = np.load(sample['motion_path'])\n        \n        # íŒ¨ë”© ë˜ëŠ” ìë¥´ê¸°\n        if motion.shape[0] < self.max_length:\n            padding = np.zeros((self.max_length - motion.shape[0], *motion.shape[1:]))\n            motion = np.concatenate([motion, padding], axis=0)\n        else:\n            motion = motion[:self.max_length]\n        \n        return {\n            'motion': torch.FloatTensor(motion),\n            'caption': sample['caption'],\n            'length': min(sample['frames'], self.max_length)\n        }\n\ntrain_dataset = MotionDataset(dataset, max_length=config.max_motion_length)\ntrain_loader = DataLoader(\n    train_dataset,\n    batch_size=config.batch_size,\n    shuffle=True,\n    num_workers=2,\n    pin_memory=True\n)\n\nprint(f\"âœ… ë°ì´í„°ë¡œë” ìƒì„± ì™„ë£Œ\")\nprint(f\"   Batch ìˆ˜: {len(train_loader)}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "create_model"
   },
   "outputs": [],
   "source": [
    "# ê°„ë‹¨í•œ ëª¨ì…˜ ìƒì„± ëª¨ë¸ (ì‹¤ì œë¡œëŠ” MDM ì‚¬ìš©)\n",
    "# ì—¬ê¸°ì„œëŠ” ë°ëª¨ìš© ê°„ë‹¨í•œ ëª¨ë¸ ì‚¬ìš©\n",
    "import torch.nn as nn\n",
    "\n",
    "class SimpleMotionModel(nn.Module):\n",
    "    def __init__(self, input_dim=66, hidden_dim=512, output_dim=66):\n",
    "        super().__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, output_dim)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x: [batch, frames, joints, 3]\n",
    "        batch, frames, joints, dims = x.shape\n",
    "        x = x.reshape(batch, frames, -1)  # [batch, frames, joints*3]\n",
    "        \n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        \n",
    "        decoded = decoded.reshape(batch, frames, joints, dims)\n",
    "        return decoded\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = SimpleMotionModel().to(device)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=config.learning_rate, weight_decay=0.01)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=config.num_epochs)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "print(\"âœ… ëª¨ë¸ ìƒì„± ì™„ë£Œ\")\n",
    "print(f\"   íŒŒë¼ë¯¸í„° ìˆ˜: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "print(f\"   ë””ë°”ì´ìŠ¤: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "training_loop"
   },
   "outputs": [],
   "source": [
    "# íŒŒì¸íŠœë‹ ì‹œì‘!\n",
    "from tqdm.notebook import tqdm\n",
    "from datetime import datetime\n",
    "\n",
    "print(\"ğŸš€ íŒŒì¸íŠœë‹ ì‹œì‘!\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "train_losses = []\n",
    "global_step = 0\n",
    "best_loss = float('inf')\n",
    "\n",
    "for epoch in range(config.num_epochs):\n",
    "    model.train()\n",
    "    epoch_loss = 0.0\n",
    "    num_batches = 0\n",
    "    \n",
    "    pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{config.num_epochs}\")\n",
    "    \n",
    "    for batch in pbar:\n",
    "        motion = batch['motion'].to(device)\n",
    "        \n",
    "        # Forward\n",
    "        optimizer.zero_grad()\n",
    "        output = model(motion)\n",
    "        loss = criterion(output, motion)\n",
    "        \n",
    "        # Backward\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        \n",
    "        # ë¡œê·¸\n",
    "        epoch_loss += loss.item()\n",
    "        num_batches += 1\n",
    "        global_step += 1\n",
    "        \n",
    "        pbar.set_postfix({'loss': f\"{loss.item():.4f}\"})\n",
    "        \n",
    "        # ì£¼ê¸°ì  ì €ì¥\n",
    "        if global_step % config.save_interval == 0:\n",
    "            checkpoint_path = f\"{CHECKPOINT_DIR}/checkpoint_epoch{epoch}_step{global_step}.pt\"\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'step': global_step,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'loss': loss.item(),\n",
    "            }, checkpoint_path)\n",
    "            print(f\"\\nğŸ’¾ ì²´í¬í¬ì¸íŠ¸ ì €ì¥: {checkpoint_path}\")\n",
    "    \n",
    "    # Epoch ì¢…ë£Œ\n",
    "    avg_epoch_loss = epoch_loss / num_batches\n",
    "    train_losses.append(avg_epoch_loss)\n",
    "    scheduler.step()\n",
    "    \n",
    "    print(f\"\\nEpoch {epoch+1} ì™„ë£Œ - Avg Loss: {avg_epoch_loss:.4f}, LR: {scheduler.get_last_lr()[0]:.6f}\")\n",
    "    \n",
    "    # ìµœê³  ëª¨ë¸ ì €ì¥\n",
    "    if avg_epoch_loss < best_loss:\n",
    "        best_loss = avg_epoch_loss\n",
    "        best_model_path = f\"{CHECKPOINT_DIR}/best_model.pt\"\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'loss': best_loss,\n",
    "        }, best_model_path)\n",
    "        print(f\"â­ ìµœê³  ëª¨ë¸ ì €ì¥! Loss: {best_loss:.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"âœ… íŒŒì¸íŠœë‹ ì™„ë£Œ!\")\n",
    "print(f\"ğŸ“Š ìµœì¢… Loss: {train_losses[-1]:.4f}\")\n",
    "print(f\"â­ ìµœê³  Loss: {best_loss:.4f}\")\n",
    "print(f\"ğŸ’¾ ì²´í¬í¬ì¸íŠ¸ ìœ„ì¹˜: {CHECKPOINT_DIR}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "section_4"
   },
   "source": [
    "---\n",
    "## 4ï¸âƒ£ í•™ìŠµ ê²°ê³¼ ì‹œê°í™”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "plot_results"
   },
   "outputs": [],
   "source": [
    "# í•™ìŠµ ê³¡ì„ \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(train_losses)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss')\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(train_losses)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss (log scale)')\n",
    "plt.title('Training Loss (Log Scale)')\n",
    "plt.yscale('log')\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{PROJECT_DIR}/training_curve.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"âœ… ê·¸ë˜í”„ ì €ì¥: {PROJECT_DIR}/training_curve.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "section_5"
   },
   "source": [
    "---\n",
    "## 5ï¸âƒ£ ì²´í¬í¬ì¸íŠ¸ ë‹¤ìš´ë¡œë“œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "list_checkpoints"
   },
   "outputs": [],
   "source": [
    "# ì €ì¥ëœ ì²´í¬í¬ì¸íŠ¸ ëª©ë¡\n",
    "import glob\n",
    "from datetime import datetime\n",
    "\n",
    "checkpoints = sorted(glob.glob(f\"{CHECKPOINT_DIR}/*.pt\"), \n",
    "                    key=os.path.getmtime, reverse=True)\n",
    "\n",
    "print(\"ğŸ“ ì €ì¥ëœ ì²´í¬í¬ì¸íŠ¸:\")\n",
    "print(\"=\"*60)\n",
    "for i, cp in enumerate(checkpoints[:10]):\n",
    "    size = os.path.getsize(cp) / (1024**2)\n",
    "    mtime = datetime.fromtimestamp(os.path.getmtime(cp))\n",
    "    name = Path(cp).name\n",
    "    print(f\"{i+1:2d}. {name:40s} ({size:6.1f} MB) - {mtime.strftime('%Y-%m-%d %H:%M')}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "download_best"
   },
   "outputs": [],
   "source": [
    "# ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ\n",
    "best_model_path = f\"{CHECKPOINT_DIR}/best_model.pt\"\n",
    "\n",
    "if os.path.exists(best_model_path):\n",
    "    print(f\"ğŸ’¾ ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì¤‘...\")\n",
    "    files.download(best_model_path)\n",
    "    print(\"âœ… ë‹¤ìš´ë¡œë“œ ì™„ë£Œ!\")\n",
    "else:\n",
    "    print(\"âŒ ìµœê³  ëª¨ë¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "section_summary"
   },
   "source": [
    "---\n",
    "## ğŸ“Š ìš”ì•½\n",
    "\n",
    "### âœ… ì™„ë£Œëœ ì‘ì—…\n",
    "1. K-pop ì˜ìƒ ì—…ë¡œë“œ ë° ë¶„ì„\n",
    "2. í¬ì¦ˆ ì¶”ì¶œ ë° HumanML3D ë³€í™˜\n",
    "3. ë°ì´í„°ì…‹ ìë™ ìƒì„±\n",
    "4. ëª¨ë¸ íŒŒì¸íŠœë‹\n",
    "5. Google Driveì— ì²´í¬í¬ì¸íŠ¸ ì €ì¥\n",
    "\n",
    "### ğŸ“ ì €ì¥ ìœ„ì¹˜\n",
    "- **í”„ë¡œì íŠ¸**: `/content/drive/MyDrive/KPop_AI_Studio/`\n",
    "- **ì²´í¬í¬ì¸íŠ¸**: `checkpoints/`\n",
    "- **í•™ìŠµ ë°ì´í„°**: `training_data/`\n",
    "\n",
    "### ğŸš€ ë‹¤ìŒ ë‹¨ê³„\n",
    "1. ë‹¤ìš´ë¡œë“œí•œ `best_model.pt`ë¥¼ ë¡œì»¬ í”„ë¡œì íŠ¸ì—ì„œ ì‚¬ìš©\n",
    "2. ë” ë§ì€ K-pop ì˜ìƒìœ¼ë¡œ ì¶”ê°€ íŒŒì¸íŠœë‹\n",
    "3. í”„ë¡œë•ì…˜ ë°°í¬\n",
    "\n",
    "### ğŸ’¡ íŒ\n",
    "- ì˜ìƒì´ ë§ì„ìˆ˜ë¡ ë” ì¢‹ì€ ê²°ê³¼\n",
    "- ë‹¤ì–‘í•œ ìŠ¤íƒ€ì¼ì˜ ì•ˆë¬´ë¥¼ í¬í•¨í•˜ë©´ ë²”ìš©ì„± í–¥ìƒ\n",
    "- GPU ë©”ëª¨ë¦¬ ë¶€ì¡± ì‹œ batch_size ì¤„ì´ê¸°"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "MDM_Finetuning_Colab.ipynb",
   "provenance": [],
   "gpuType": "T4"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}