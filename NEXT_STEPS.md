# ë‹¤ìŒ ë‹¨ê³„ êµ¬í˜„ ê°€ì´ë“œ

## ğŸ¯ ëª©í‘œ
ìŒì•… + í…ìŠ¤íŠ¸ í”„ë¡¬í”„íŠ¸ â†’ K-pop ì•ˆë¬´ ìƒì„± AI ì‹œìŠ¤í…œ ì™„ì„±

---

## ğŸ“‹ Phase 1: ë°±ì—”ë“œ ì¸í”„ë¼ êµ¬ì¶• (1-2ì£¼)

### 1.1 FastAPI ì„œë²„ ì„¤ì •

```python
# backend/main.py
from fastapi import FastAPI, File, UploadFile, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import StreamingResponse
import uvicorn

app = FastAPI(title="K-Pop Motion Generation API")

# CORS ì„¤ì •
app.add_middleware(
    CORSMiddleware,
    allow_origins=["http://localhost:5173", "http://localhost:3000"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

@app.get("/")
async def root():
    return {"status": "K-Pop Motion Generation API"}

@app.post("/api/analyze-audio")
async def analyze_audio(audio_file: UploadFile = File(...)):
    """
    ì˜¤ë””ì˜¤ íŒŒì¼ ë¶„ì„
    - í…œí¬ (BPM)
    - ë¹„íŠ¸ íƒ€ì„ìŠ¤íƒ¬í”„
    - ì—ë„ˆì§€ ë ˆë²¨
    - í‚¤ ì •ë³´
    """
    # TODO: ì˜¤ë””ì˜¤ ë¶„ì„ ë¡œì§
    pass

@app.post("/api/generate-motion")
async def generate_motion(
    prompt: str,
    audio_file: UploadFile = File(...),
    style: str = "hiphop",
    energy: float = 0.75,
    smoothness: float = 0.5,
    bounce: float = 0.6,
    creativity: float = 0.4
):
    """
    ìŒì•… + í”„ë¡¬í”„íŠ¸ë¡œ ì•ˆë¬´ ìƒì„±
    """
    # TODO: ëª¨ì…˜ ìƒì„± ë¡œì§
    pass

@app.get("/api/generation-status/{job_id}")
async def get_generation_status(job_id: str):
    """
    ìƒì„± ì‘ì—… ìƒíƒœ ì¡°íšŒ
    """
    pass

if __name__ == "__main__":
    uvicorn.run(app, host="0.0.0.0", port=8000)
```

### 1.2 í”„ë¡œì íŠ¸ êµ¬ì¡°

```
backend/
â”œâ”€â”€ main.py                 # FastAPI ì•±
â”œâ”€â”€ requirements.txt        # ì˜ì¡´ì„±
â”œâ”€â”€ services/
â”‚   â”œâ”€â”€ audio_processor.py # ì˜¤ë””ì˜¤ ë¶„ì„
â”‚   â”œâ”€â”€ motion_generator.py # ëª¨ì…˜ ìƒì„±
â”‚   â””â”€â”€ model_loader.py    # ëª¨ë¸ ë¡œë”©
â”œâ”€â”€ models/
â”‚   â””â”€â”€ (ë‹¤ìš´ë¡œë“œëœ ëª¨ë¸ íŒŒì¼ë“¤)
â”œâ”€â”€ utils/
â”‚   â”œâ”€â”€ audio_utils.py
â”‚   â””â”€â”€ motion_utils.py
â””â”€â”€ config.py              # ì„¤ì • íŒŒì¼
```

### 1.3 requirements.txt

```txt
fastapi==0.104.1
uvicorn[standard]==0.24.0
python-multipart==0.0.6
numpy==1.24.3
torch==2.1.0
torchaudio==2.1.0
librosa==0.10.1
madmom==0.16.1
scipy==1.11.4
Pillow==10.1.0
```

---

## ğŸ¤– Phase 2: AI ëª¨ë¸ ì„ íƒ ë° í†µí•© (2-3ì£¼)

### 2.1 ëª¨ì…˜ ìƒì„± ëª¨ë¸ ì˜µì…˜

#### ì˜µì…˜ A: MDM (Motion Diffusion Model) - ì¶”ì²œ â­
**ì¥ì :**
- Text-to-Motion ìƒì„±ì— ìµœì í™”
- Diffusion ê¸°ë°˜ìœ¼ë¡œ ê³ í’ˆì§ˆ ìƒì„±
- HumanML3D ë°ì´í„°ì…‹ìœ¼ë¡œ í•™ìŠµë¨
- ì˜¤í”ˆì†ŒìŠ¤ (GitHubì—ì„œ ë‹¤ìš´ë¡œë“œ ê°€ëŠ¥)

**ë‹¨ì :**
- K-pop íŠ¹í™” ë°ì´í„°ë¡œ Fine-tuning í•„ìš”
- ì˜¤ë””ì˜¤ ë™ê¸°í™” ê¸°ëŠ¥ ì—†ìŒ (ë³„ë„ êµ¬í˜„ í•„ìš”)

**í†µí•© ë°©ë²•:**
```python
# backend/services/motion_generator.py
import torch
from mdm.model import MDM
from mdm.sample import generate

class MotionGenerator:
    def __init__(self):
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        self.model = self.load_mdm_model()
        
    def load_mdm_model(self):
        # MDM ëª¨ë¸ ë¡œë“œ
        model_path = "models/mdm_humanml3d_ft/model.npz"
        model = MDM(...)
        # ê°€ì¤‘ì¹˜ ë¡œë“œ
        return model.to(self.device)
    
    def generate(self, prompt, style, audio_features=None):
        # 1. í”„ë¡¬í”„íŠ¸ ì„ë² ë”©
        # 2. ìŠ¤íƒ€ì¼ ì¡°ê±´ë¶€ ìƒì„±
        # 3. ëª¨ì…˜ ìƒì„±
        motion = generate(
            model=self.model,
            caption=prompt,
            length=audio_features['duration'] if audio_features else 10.0,
            ...
        )
        return motion
```

**ë¦¬ì†ŒìŠ¤:**
- GitHub: https://github.com/GuyTevet/motion-diffusion-model
- ë…¼ë¬¸: "Human Motion Diffusion Model"
- ì‚¬ì „ í•™ìŠµ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ í•„ìš”

#### ì˜µì…˜ B: T2M-GPT
**ì¥ì :**
- Transformer ê¸°ë°˜ìœ¼ë¡œ ê¸´ ì‹œí€€ìŠ¤ ìƒì„± ê°€ëŠ¥
- ByteDanceì—ì„œ ê°œë°œ

**ë‹¨ì :**
- ì½”ë“œ/ëª¨ë¸ ì ‘ê·¼ì„± ë‚®ì„ ìˆ˜ ìˆìŒ

#### ì˜µì…˜ C: MotionGPT
**ì¥ì :**
- LLM ê¸°ë°˜ìœ¼ë¡œ ìì—°ì–´ ì´í•´ ìš°ìˆ˜

**ë‹¨ì :**
- ìµœì‹  ëª¨ë¸ì´ë¼ ì•ˆì •ì„± ê²€ì¦ í•„ìš”

### 2.2 ì˜¤ë””ì˜¤ ë¶„ì„ ëª¨ë¸

```python
# backend/services/audio_processor.py
import librosa
import madmom
import numpy as np

class AudioProcessor:
    def analyze(self, audio_path):
        """
        ì˜¤ë””ì˜¤ íŒŒì¼ ë¶„ì„
        """
        # 1. í…œí¬ ì¶”ì •
        tempo, beats = librosa.beat.beat_track(
            y=audio, sr=sr, units='time'
        )
        
        # 2. ë¹„íŠ¸ ê°ì§€ (ë” ì •í™•)
        proc = madmom.features.beats.DBNBeatTrackingProcessor(fps=100)
        act = madmom.features.beats.RNNBeatProcessor()(audio_path)
        beats = proc(act)
        
        # 3. ì—ë„ˆì§€ ê³„ì‚°
        energy = np.mean(librosa.feature.rms(y=audio))
        
        # 4. í‚¤ ì¶”ì •
        key = self.estimate_key(audio)
        
        return {
            'tempo': float(tempo),
            'beats': beats.tolist(),
            'energy': float(energy),
            'duration': float(librosa.get_duration(y=audio, sr=sr)),
            'key': key
        }
```

### 2.3 ì˜¤ë””ì˜¤-ëª¨ì…˜ ë™ê¸°í™”

```python
# backend/services/audio_sync.py
from scipy.interpolate import interp1d

class AudioMotionSync:
    def align_motion_to_beat(self, motion, audio_features):
        """
        ëª¨ì…˜ì„ ì˜¤ë””ì˜¤ ë¹„íŠ¸ì— ë§ì¶° ì •ë ¬
        """
        beats = audio_features['beats']
        motion_frames = len(motion)
        
        # ë¹„íŠ¸ íƒ€ì„ìŠ¤íƒ¬í”„ë¥¼ í”„ë ˆì„ ì¸ë±ìŠ¤ë¡œ ë³€í™˜
        fps = 30  # ëª¨ì…˜ FPS
        beat_frames = [int(b * fps) for b in beats]
        
        # ëª¨ì…˜ì„ ë¹„íŠ¸ì— ë§ì¶° ë¦¬ìƒ˜í”Œë§
        aligned_motion = self.resample_motion(motion, beat_frames)
        
        return aligned_motion
```

---

## ğŸ¨ Phase 3: K-pop íŠ¹í™” Fine-tuning (3-4ì£¼)

### 3.1 ë°ì´í„° ìˆ˜ì§‘

```python
# scripts/collect_kpop_data.py
import yt_dlp
import cv2
from pose_estimator import PoseEstimator

class KPopDataCollector:
    def collect_videos(self, video_urls):
        """
        YouTubeì—ì„œ K-pop ì•ˆë¬´ ì˜ìƒ ìˆ˜ì§‘
        """
        ydl_opts = {
            'format': 'bestvideo[ext=mp4]+bestaudio[ext=m4a]/best[ext=mp4]',
            'outtmpl': 'data/videos/%(title)s.%(ext)s',
        }
        
        with yt_dlp.YoutubeDL(ydl_opts) as ydl:
            for url in video_urls:
                ydl.download([url])
    
    def extract_motion(self, video_path):
        """
        ì˜ìƒì—ì„œ ëª¨ì…˜ ë°ì´í„° ì¶”ì¶œ
        """
        pose_estimator = PoseEstimator()
        motions = []
        
        cap = cv2.VideoCapture(video_path)
        while cap.isOpened():
            ret, frame = cap.read()
            if not ret:
                break
            
            # í¬ì¦ˆ ì¶”ì •
            pose = pose_estimator.estimate(frame)
            motions.append(pose)
        
        return np.array(motions)
```

### 3.2 ë°ì´í„° ì „ì²˜ë¦¬

```python
# scripts/preprocess_data.py
import numpy as np
from scipy import signal

class DataPreprocessor:
    def normalize_motion(self, motion):
        """
        ëª¨ì…˜ ë°ì´í„° ì •ê·œí™”
        - ì¢Œí‘œê³„ í†µì¼
        - ìŠ¤ì¼€ì¼ ì •ê·œí™”
        - í”„ë ˆì„ ë ˆì´íŠ¸ í†µì¼
        """
        # 1. ë£¨íŠ¸ ê´€ì ˆ(ì—‰ë©ì´)ì„ ì›ì ìœ¼ë¡œ ì´ë™
        motion = motion - motion[:, 0:1, :]  # ì²« ë²ˆì§¸ ê´€ì ˆì´ ë£¨íŠ¸
        
        # 2. ìŠ¤ì¼€ì¼ ì •ê·œí™”
        motion = motion / np.std(motion)
        
        # 3. FPS í†µì¼ (30fpsë¡œ ë¦¬ìƒ˜í”Œë§)
        if len(motion) != target_frames:
            motion = signal.resample(motion, target_frames)
        
        return motion
    
    def align_with_audio(self, motion, audio_beats):
        """
        ëª¨ì…˜ì„ ì˜¤ë””ì˜¤ ë¹„íŠ¸ì— ì •ë ¬
        """
        # Dynamic Time Warping ì‚¬ìš©
        from dtaidistance import dtw
        alignment = dtw.warp(motion, audio_beats)
        return alignment
```

### 3.3 Fine-tuning

```python
# scripts/finetune_mdm.py
import torch
from torch.utils.data import Dataset, DataLoader
from mdm.model import MDM

class KPopMotionDataset(Dataset):
    def __init__(self, motion_files, captions):
        self.motions = [np.load(f) for f in motion_files]
        self.captions = captions
    
    def __len__(self):
        return len(self.motions)
    
    def __getitem__(self, idx):
        return {
            'motion': self.motions[idx],
            'caption': self.captions[idx]
        }

def finetune_mdm():
    # 1. ì‚¬ì „ í•™ìŠµëœ MDM ëª¨ë¸ ë¡œë“œ
    model = MDM(...)
    model.load_state_dict(torch.load('models/mdm_pretrained.pth'))
    
    # 2. K-pop ë°ì´í„°ì…‹ ë¡œë“œ
    dataset = KPopMotionDataset(...)
    dataloader = DataLoader(dataset, batch_size=32)
    
    # 3. Fine-tuning
    optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)
    
    for epoch in range(10):
        for batch in dataloader:
            # Forward pass
            loss = model(batch['motion'], batch['caption'])
            
            # Backward pass
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()
    
    # 4. ëª¨ë¸ ì €ì¥
    torch.save(model.state_dict(), 'models/mdm_kpop_finetuned.pth')
```

---

## ğŸ”— Phase 4: í”„ë¡ íŠ¸ì—”ë“œ-ë°±ì—”ë“œ ì—°ë™ (1ì£¼)

### 4.1 API í´ë¼ì´ì–¸íŠ¸ ìƒì„±

```javascript
// frontend/services/api.js
const API_BASE_URL = 'http://localhost:8000';

export const analyzeAudio = async (audioFile) => {
  const formData = new FormData();
  formData.append('audio_file', audioFile);
  
  const response = await fetch(`${API_BASE_URL}/api/analyze-audio`, {
    method: 'POST',
    body: formData,
  });
  
  if (!response.ok) {
    throw new Error('Audio analysis failed');
  }
  
  return response.json();
};

export const generateMotion = async (prompt, audioFile, params) => {
  const formData = new FormData();
  formData.append('audio_file', audioFile);
  formData.append('prompt', prompt);
  formData.append('style', params.style);
  formData.append('energy', params.energy);
  formData.append('smoothness', params.smoothness);
  formData.append('bounce', params.bounce);
  formData.append('creativity', params.creativity);
  
  const response = await fetch(`${API_BASE_URL}/api/generate-motion`, {
    method: 'POST',
    body: formData,
  });
  
  if (!response.ok) {
    throw new Error('Motion generation failed');
  }
  
  return response.json();
};

export const getGenerationStatus = async (jobId) => {
  const response = await fetch(`${API_BASE_URL}/api/generation-status/${jobId}`);
  return response.json();
};
```

### 4.2 í”„ë¡ íŠ¸ì—”ë“œ ìˆ˜ì •

```javascript
// Sota_KPop_Studio.jsxì— ì¶”ê°€
import { analyzeAudio, generateMotion, getGenerationStatus } from './services/api';

// handleAudioUpload ìˆ˜ì •
const handleAudioUpload = async (e) => {
  const file = e.target.files[0];
  if (file) {
    setAudioFile(file);
    setIsAnalyzingAudio(true);
    
    try {
      const analysis = await analyzeAudio(file);
      setAudioAnalysis(analysis);
      setTotalDuration(analysis.duration);
      // ...
    } catch (error) {
      alert('ì˜¤ë””ì˜¤ ë¶„ì„ ì‹¤íŒ¨: ' + error.message);
    } finally {
      setIsAnalyzingAudio(false);
    }
  }
};

// handleGenerateMotion ìˆ˜ì •
const handleGenerateMotion = async () => {
  // ... ê²€ì¦ ì½”ë“œ ...
  
  setIsGenerating(true);
  setGenerationProgress(0);
  
  try {
    const result = await generateMotion(prompt, audioFile, {
      style: selectedStyle,
      ...params
    });
    
    // í´ë§ìœ¼ë¡œ ì§„í–‰ ìƒí™© í™•ì¸
    const pollStatus = async () => {
      const status = await getGenerationStatus(result.job_id);
      setGenerationProgress(status.progress);
      
      if (status.status === 'completed') {
        // ìƒì„±ëœ ëª¨ì…˜ ë°ì´í„°ë¥¼ íƒ€ì„ë¼ì¸ì— ì¶”ê°€
        setTracks(prev => ({
          ...prev,
          motion: [{
            id: `motion_${Date.now()}`,
            start: 0,
            duration: 100,
            name: `${selectedStyle} - ${prompt.substring(0, 15)}...`,
            active: true,
            motionData: status.motion_data, // ì‹¤ì œ ëª¨ì…˜ ë°ì´í„°
            ...
          }]
        }));
        setIsGenerating(false);
      } else if (status.status === 'processing') {
        setTimeout(pollStatus, 1000);
      }
    };
    
    pollStatus();
  } catch (error) {
    alert('ì•ˆë¬´ ìƒì„± ì‹¤íŒ¨: ' + error.message);
    setIsGenerating(false);
  }
};
```

---

## ğŸš€ Phase 5: ë°°í¬ ë° ìµœì í™” (1-2ì£¼)

### 5.1 ëª¨ë¸ ìµœì í™”

```python
# ëª¨ë¸ ì–‘ìí™” ë° ìµœì í™”
import torch.quantization as quantization

def optimize_model(model):
    # 1. ëª¨ë¸ ì–‘ìí™”
    quantized_model = quantization.quantize_dynamic(
        model, {torch.nn.Linear}, dtype=torch.qint8
    )
    
    # 2. ONNX ë³€í™˜ (ë” ë¹ ë¥¸ ì¶”ë¡ )
    torch.onnx.export(
        model,
        dummy_input,
        "models/mdm_optimized.onnx",
        opset_version=11
    )
    
    return quantized_model
```

### 5.2 GPU ì„œë²„ ì„¤ì •

```dockerfile
# Dockerfile
FROM nvidia/cuda:11.8.0-cudnn8-runtime-ubuntu22.04

WORKDIR /app

# Python ì„¤ì¹˜
RUN apt-get update && apt-get install -y python3.10 python3-pip

# ì˜ì¡´ì„± ì„¤ì¹˜
COPY requirements.txt .
RUN pip install -r requirements.txt

# ëª¨ë¸ íŒŒì¼ ë³µì‚¬
COPY models/ ./models/
COPY backend/ ./backend/

# ì„œë²„ ì‹¤í–‰
CMD ["uvicorn", "backend.main:app", "--host", "0.0.0.0", "--port", "8000"]
```

### 5.3 ë°°í¬ ì˜µì…˜

1. **ë¡œì»¬ ê°œë°œ**: `uvicorn backend.main:app --reload`
2. **í´ë¼ìš°ë“œ (AWS/GCP)**: GPU ì¸ìŠ¤í„´ìŠ¤ì— ë°°í¬
3. **Docker**: ì»¨í…Œì´ë„ˆí™”í•˜ì—¬ ë°°í¬
4. **Edge ë°°í¬**: ONNX Runtimeìœ¼ë¡œ ëª¨ë°”ì¼/ì—£ì§€ ë””ë°”ì´ìŠ¤

---

## ğŸ“Š ìš°ì„ ìˆœìœ„ ë° íƒ€ì„ë¼ì¸

### Week 1-2: ë°±ì—”ë“œ ì¸í”„ë¼
- [ ] FastAPI ì„œë²„ êµ¬ì¶•
- [ ] ì˜¤ë””ì˜¤ ë¶„ì„ íŒŒì´í”„ë¼ì¸
- [ ] ê¸°ë³¸ API ì—”ë“œí¬ì¸íŠ¸

### Week 3-4: ëª¨ë¸ í†µí•©
- [ ] MDM ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ë° ë¡œë“œ
- [ ] ê¸°ë³¸ Text-to-Motion ìƒì„± í…ŒìŠ¤íŠ¸
- [ ] ì˜¤ë””ì˜¤ ë¶„ì„ í†µí•©

### Week 5-6: ì˜¤ë””ì˜¤-ëª¨ì…˜ ë™ê¸°í™”
- [ ] ë¹„íŠ¸ ê°ì§€ ë° ì •ë ¬
- [ ] ëª¨ì…˜ ë¦¬ìƒ˜í”Œë§
- [ ] ë™ê¸°í™” í…ŒìŠ¤íŠ¸

### Week 7-10: Fine-tuning
- [ ] K-pop ë°ì´í„° ìˆ˜ì§‘
- [ ] ë°ì´í„° ì „ì²˜ë¦¬
- [ ] Fine-tuning ì‹¤í–‰
- [ ] ëª¨ë¸ í‰ê°€

### Week 11: í”„ë¡ íŠ¸ì—”ë“œ ì—°ë™
- [ ] API í´ë¼ì´ì–¸íŠ¸ êµ¬í˜„
- [ ] ì‹¤ì‹œê°„ ìƒíƒœ ì—…ë°ì´íŠ¸
- [ ] ì—ëŸ¬ ì²˜ë¦¬

### Week 12: ë°°í¬ ë° ìµœì í™”
- [ ] ëª¨ë¸ ìµœì í™”
- [ ] ì„±ëŠ¥ íŠœë‹
- [ ] ë°°í¬

---

## ğŸ¯ ë¹ ë¥¸ ì‹œì‘ (MVP)

ìµœì†Œ ê¸°ëŠ¥ìœ¼ë¡œ ë¹ ë¥´ê²Œ ì‹œì‘í•˜ë ¤ë©´:

1. **MDM ëª¨ë¸ ë‹¤ìš´ë¡œë“œ** (ì‚¬ì „ í•™ìŠµëœ ëª¨ë¸ ì‚¬ìš©)
2. **ì˜¤ë””ì˜¤ ë¶„ì„ë§Œ êµ¬í˜„** (librosa ì‚¬ìš©)
3. **ê¸°ë³¸ Text-to-Motion ìƒì„±** (ì˜¤ë””ì˜¤ ë™ê¸°í™”ëŠ” ë‚˜ì¤‘ì—)
4. **í”„ë¡ íŠ¸ì—”ë“œ ì—°ë™**

ì´ë ‡ê²Œ í•˜ë©´ 2-3ì£¼ ì•ˆì— ê¸°ë³¸ ë²„ì „ì„ ë§Œë“¤ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

---

## ğŸ“š ì°¸ê³  ë¦¬ì†ŒìŠ¤

### ëª¨ë¸
- MDM: https://github.com/GuyTevet/motion-diffusion-model
- HumanML3D Dataset: https://github.com/EricGuo5513/HumanML3D

### ì˜¤ë””ì˜¤ ì²˜ë¦¬
- Librosa: https://librosa.org/
- Madmom: https://github.com/CPJKU/madmom

### í¬ì¦ˆ ì¶”ì •
- MediaPipe: https://mediapipe.dev/
- OpenPose: https://github.com/CMU-Perceptual-Computing-Lab/openpose

### í•™ìŠµ ë°ì´í„°
- AIST++: ëŒ„ìŠ¤ ë°ì´í„°ì…‹
- YouTube K-pop ì•ˆë¬´ ì˜ìƒ

---

## ğŸ’¡ íŒ

1. **ì ì§„ì  ê°œë°œ**: ë¨¼ì € ê¸°ë³¸ ê¸°ëŠ¥ë¶€í„°, ë‚˜ì¤‘ì— ê³ ê¸‰ ê¸°ëŠ¥ ì¶”ê°€
2. **ëª¨ë¸ ìºì‹±**: ëª¨ë¸ì„ í•œ ë²ˆë§Œ ë¡œë“œí•˜ê³  ì¬ì‚¬ìš©
3. **ë¹„ë™ê¸° ì²˜ë¦¬**: ê¸´ ì‘ì—…ì€ ë°±ê·¸ë¼ìš´ë“œì—ì„œ ì²˜ë¦¬
4. **ì—ëŸ¬ ì²˜ë¦¬**: ëª¨ë“  API í˜¸ì¶œì— try-catch ì¶”ê°€
5. **ë¡œê¹…**: ë””ë²„ê¹…ì„ ìœ„í•´ ìƒì„¸í•œ ë¡œê·¸ ë‚¨ê¸°ê¸°

