"""
ëª¨ë¸ ë‚´ë³´ë‚´ê¸° ìŠ¤í¬ë¦½íŠ¸
ONNX ë° TorchScript í˜•ì‹ìœ¼ë¡œ ëª¨ë¸ ë³€í™˜
EDGE ìµœì í™”ë¥¼ ìœ„í•œ ì–‘ìí™” ë° í”„ë£¨ë‹ ì§€ì›
"""
import os
import sys
import json
import torch
import argparse
from pathlib import Path
from typing import Optional, Dict, Any

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê²½ë¡œ ì¶”ê°€
base_dir = Path(__file__).parent.parent
sys.path.insert(0, str(base_dir))


class ModelExporter:
    """
    MDM ëª¨ë¸ì„ ë‹¤ì–‘í•œ í˜•ì‹ìœ¼ë¡œ ë‚´ë³´ëƒ…ë‹ˆë‹¤.
    """

    def __init__(self, model_path: str, config_path: Optional[str] = None):
        """
        Args:
            model_path: ì›ë³¸ ëª¨ë¸ ì²´í¬í¬ì¸íŠ¸ ê²½ë¡œ
            config_path: EDGE ì„¤ì • íŒŒì¼ ê²½ë¡œ
        """
        self.model_path = Path(model_path)
        self.config = self._load_config(config_path)
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        self.model = None

    def _load_config(self, config_path: Optional[str]) -> Dict:
        """EDGE ì„¤ì • ë¡œë“œ"""
        if config_path is None:
            config_path = base_dir / "edge_config.json"

        if os.path.exists(config_path):
            with open(config_path, 'r') as f:
                return json.load(f)
        else:
            # ê¸°ë³¸ ì„¤ì •
            return {
                "optimization": {
                    "quantization": {"enabled": True, "dtype": "int8"},
                    "pruning": {"enabled": False, "sparsity": 0.3}
                },
                "export": {
                    "formats": ["onnx", "torchscript"],
                    "onnx": {
                        "opset_version": 14,
                        "dynamic_axes": {
                            "input": {"0": "batch_size", "3": "sequence_length"},
                            "output": {"0": "batch_size", "3": "sequence_length"}
                        }
                    },
                    "torchscript": {"method": "trace"}
                }
            }

    def load_model(self):
        """ëª¨ë¸ ë¡œë“œ"""
        try:
            # MDM ëª¨ë¸ ë¡œë“œ ë¡œì§
            # ì‹¤ì œ êµ¬í˜„ì€ MDM ì €ì¥ì†Œì˜ ì½”ë“œ ì‚¬ìš©
            from services.mdm_integration import MDMIntegration

            print(f"ğŸ“¥ ëª¨ë¸ ë¡œë“œ ì¤‘: {self.model_path}")

            # args.json ê²½ë¡œ ì°¾ê¸°
            args_path = self.model_path.parent / "args.json"
            if not args_path.exists():
                raise FileNotFoundError(f"args.jsonì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {args_path}")

            # MDM í†µí•© ì‚¬ìš©
            mdm_integration = MDMIntegration(str(self.model_path), str(args_path))
            if not mdm_integration.load_model():
                raise RuntimeError("ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨")

            self.model = mdm_integration.model
            self.diffusion = mdm_integration.diffusion
            self.args = mdm_integration.args

            print(f"âœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
            print(f"   íŒŒë¼ë¯¸í„° ìˆ˜: {sum(p.numel() for p in self.model.parameters()):,}")

            return True

        except Exception as e:
            print(f"âŒ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
            return False

    def apply_quantization(self, output_path: str):
        """
        ë™ì  ì–‘ìí™” ì ìš©

        Args:
            output_path: ì–‘ìí™”ëœ ëª¨ë¸ ì €ì¥ ê²½ë¡œ
        """
        if not self.config["optimization"]["quantization"]["enabled"]:
            print("â­ï¸  ì–‘ìí™”ê°€ ë¹„í™œì„±í™”ë˜ì–´ ìˆìŠµë‹ˆë‹¤.")
            return None

        print("ğŸ”§ ë™ì  ì–‘ìí™” ì ìš© ì¤‘...")

        try:
            # CPUë¡œ ì´ë™ (ì–‘ìí™”ëŠ” CPUì—ì„œë§Œ ê°€ëŠ¥)
            model_cpu = self.model.cpu()

            # ë™ì  ì–‘ìí™”
            quantized_model = torch.quantization.quantize_dynamic(
                model_cpu,
                {torch.nn.Linear, torch.nn.LSTM, torch.nn.GRU},
                dtype=torch.qint8
            )

            # ì €ì¥
            output_path = Path(output_path)
            output_path.parent.mkdir(parents=True, exist_ok=True)

            torch.save({
                'model_state_dict': quantized_model.state_dict(),
                'quantized': True,
                'dtype': 'qint8'
            }, output_path)

            # í¬ê¸° ë¹„êµ
            original_size = os.path.getsize(self.model_path) / (1024 ** 2)
            quantized_size = os.path.getsize(output_path) / (1024 ** 2)

            print(f"âœ… ì–‘ìí™” ì™„ë£Œ: {output_path}")
            print(f"   ì›ë³¸ í¬ê¸°: {original_size:.2f} MB")
            print(f"   ì–‘ìí™” í¬ê¸°: {quantized_size:.2f} MB")
            print(f"   ì••ì¶•ë¥ : {(1 - quantized_size / original_size) * 100:.1f}%")

            # ëª¨ë¸ì„ ë‹¤ì‹œ ì›ë˜ ë””ë°”ì´ìŠ¤ë¡œ ì´ë™
            self.model.to(self.device)

            return str(output_path)

        except Exception as e:
            print(f"âŒ ì–‘ìí™” ì‹¤íŒ¨: {e}")
            return None

    def export_to_onnx(self, output_path: str):
        """
        ONNX í˜•ì‹ìœ¼ë¡œ ë‚´ë³´ë‚´ê¸°

        Args:
            output_path: ONNX ëª¨ë¸ ì €ì¥ ê²½ë¡œ
        """
        if "onnx" not in self.config["export"]["formats"]:
            print("â­ï¸  ONNX ë‚´ë³´ë‚´ê¸°ê°€ ë¹„í™œì„±í™”ë˜ì–´ ìˆìŠµë‹ˆë‹¤.")
            return None

        print("ğŸ“¦ ONNX í˜•ì‹ìœ¼ë¡œ ë³€í™˜ ì¤‘...")

        try:
            # ë”ë¯¸ ì…ë ¥ ìƒì„±
            batch_size = 1
            num_features = self.model.nfeats
            num_joints = self.model.njoints
            seq_len = 196  # HumanML3D ê¸°ë³¸ í”„ë ˆì„ ìˆ˜

            # ëª¨ë¸ ì…ë ¥ í˜•ì‹ì— ë§ê²Œ ì¡°ì •
            dummy_input = torch.randn(batch_size, num_joints, num_features, seq_len).to(self.device)

            # CPUë¡œ ì´ë™
            self.model.cpu()
            dummy_input = dummy_input.cpu()

            # ONNX ë‚´ë³´ë‚´ê¸°
            output_path = Path(output_path)
            output_path.parent.mkdir(parents=True, exist_ok=True)

            onnx_config = self.config["export"]["onnx"]

            torch.onnx.export(
                self.model,
                dummy_input,
                str(output_path),
                export_params=True,
                opset_version=onnx_config["opset_version"],
                do_constant_folding=True,
                input_names=['motion_input'],
                output_names=['motion_output'],
                dynamic_axes=onnx_config.get("dynamic_axes", None)
            )

            file_size = os.path.getsize(output_path) / (1024 ** 2)
            print(f"âœ… ONNX ë³€í™˜ ì™„ë£Œ: {output_path}")
            print(f"   íŒŒì¼ í¬ê¸°: {file_size:.2f} MB")

            # ëª¨ë¸ì„ ë‹¤ì‹œ ì›ë˜ ë””ë°”ì´ìŠ¤ë¡œ ì´ë™
            self.model.to(self.device)

            # ONNX ëª¨ë¸ ê²€ì¦
            self._verify_onnx(output_path)

            return str(output_path)

        except Exception as e:
            print(f"âŒ ONNX ë³€í™˜ ì‹¤íŒ¨: {e}")
            import traceback
            traceback.print_exc()
            return None

    def export_to_torchscript(self, output_path: str):
        """
        TorchScript í˜•ì‹ìœ¼ë¡œ ë‚´ë³´ë‚´ê¸°

        Args:
            output_path: TorchScript ëª¨ë¸ ì €ì¥ ê²½ë¡œ
        """
        if "torchscript" not in self.config["export"]["formats"]:
            print("â­ï¸  TorchScript ë‚´ë³´ë‚´ê¸°ê°€ ë¹„í™œì„±í™”ë˜ì–´ ìˆìŠµë‹ˆë‹¤.")
            return None

        print("ğŸ“¦ TorchScript í˜•ì‹ìœ¼ë¡œ ë³€í™˜ ì¤‘...")

        try:
            # ë”ë¯¸ ì…ë ¥ ìƒì„±
            batch_size = 1
            num_features = self.model.nfeats
            num_joints = self.model.njoints
            seq_len = 196

            dummy_input = torch.randn(batch_size, num_joints, num_features, seq_len).to(self.device)

            # í‰ê°€ ëª¨ë“œ
            self.model.eval()

            # TorchScript ë³€í™˜ ë°©ë²• ì„ íƒ
            method = self.config["export"]["torchscript"]["method"]

            if method == "trace":
                # Tracing
                print("   ë°©ë²•: Tracing")
                scripted_model = torch.jit.trace(self.model, dummy_input)
            else:
                # Scripting
                print("   ë°©ë²•: Scripting")
                scripted_model = torch.jit.script(self.model)

            # ì €ì¥
            output_path = Path(output_path)
            output_path.parent.mkdir(parents=True, exist_ok=True)

            scripted_model.save(str(output_path))

            file_size = os.path.getsize(output_path) / (1024 ** 2)
            print(f"âœ… TorchScript ë³€í™˜ ì™„ë£Œ: {output_path}")
            print(f"   íŒŒì¼ í¬ê¸°: {file_size:.2f} MB")

            return str(output_path)

        except Exception as e:
            print(f"âŒ TorchScript ë³€í™˜ ì‹¤íŒ¨: {e}")
            import traceback
            traceback.print_exc()
            return None

    def _verify_onnx(self, onnx_path: Path):
        """ONNX ëª¨ë¸ ê²€ì¦"""
        try:
            import onnx
            import onnxruntime as ort

            # ONNX ëª¨ë¸ ë¡œë“œ ë° ê²€ì¦
            onnx_model = onnx.load(str(onnx_path))
            onnx.checker.check_model(onnx_model)
            print("   âœ“ ONNX ëª¨ë¸ ê²€ì¦ ì™„ë£Œ")

            # ONNX Runtimeìœ¼ë¡œ ì¶”ë¡  í…ŒìŠ¤íŠ¸
            ort_session = ort.InferenceSession(str(onnx_path))
            print("   âœ“ ONNX Runtime ë¡œë“œ ì„±ê³µ")

        except ImportError:
            print("   âš ï¸  onnx ë˜ëŠ” onnxruntimeì´ ì„¤ì¹˜ë˜ì§€ ì•Šì•„ ê²€ì¦ì„ ê±´ë„ˆëœë‹ˆë‹¤.")
        except Exception as e:
            print(f"   âš ï¸  ONNX ê²€ì¦ ì‹¤íŒ¨: {e}")

    def export_all(self, output_dir: str):
        """
        ëª¨ë“  í˜•ì‹ìœ¼ë¡œ ë‚´ë³´ë‚´ê¸°

        Args:
            output_dir: ì¶œë ¥ ë””ë ‰í† ë¦¬
        """
        output_dir = Path(output_dir)
        output_dir.mkdir(parents=True, exist_ok=True)

        results = {}

        # ì–‘ìí™”
        if self.config["optimization"]["quantization"]["enabled"]:
            quantized_path = output_dir / "model_quantized.pt"
            results['quantized'] = self.apply_quantization(str(quantized_path))

        # ONNX
        if "onnx" in self.config["export"]["formats"]:
            onnx_path = output_dir / "model.onnx"
            results['onnx'] = self.export_to_onnx(str(onnx_path))

        # TorchScript
        if "torchscript" in self.config["export"]["formats"]:
            torchscript_path = output_dir / "model_scripted.pt"
            results['torchscript'] = self.export_to_torchscript(str(torchscript_path))

        # ê²°ê³¼ ìš”ì•½
        print("\n" + "=" * 60)
        print("ë‚´ë³´ë‚´ê¸° ì™„ë£Œ ìš”ì•½:")
        print("=" * 60)
        for format_name, path in results.items():
            if path:
                print(f"âœ… {format_name.upper()}: {path}")
            else:
                print(f"âŒ {format_name.upper()}: ì‹¤íŒ¨")
        print("=" * 60)

        return results


def main():
    parser = argparse.ArgumentParser(description="MDM ëª¨ë¸ ë‚´ë³´ë‚´ê¸°")
    parser.add_argument(
        "--model-path",
        type=str,
        required=True,
        help="ì›ë³¸ ëª¨ë¸ ì²´í¬í¬ì¸íŠ¸ ê²½ë¡œ (.pt íŒŒì¼)"
    )
    parser.add_argument(
        "--output-dir",
        type=str,
        default="exported_models",
        help="ì¶œë ¥ ë””ë ‰í† ë¦¬"
    )
    parser.add_argument(
        "--config",
        type=str,
        default=None,
        help="EDGE ì„¤ì • íŒŒì¼ ê²½ë¡œ (ê¸°ë³¸ê°’: edge_config.json)"
    )
    parser.add_argument(
        "--format",
        type=str,
        choices=["onnx", "torchscript", "quantized", "all"],
        default="all",
        help="ë‚´ë³´ë‚¼ í˜•ì‹"
    )

    args = parser.parse_args()

    # ëª¨ë¸ ê²½ë¡œ í™•ì¸
    if not os.path.exists(args.model_path):
        print(f"âŒ ëª¨ë¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {args.model_path}")
        return

    # Exporter ìƒì„±
    exporter = ModelExporter(args.model_path, args.config)

    # ëª¨ë¸ ë¡œë“œ
    if not exporter.load_model():
        print("âŒ ëª¨ë¸ ë¡œë“œì— ì‹¤íŒ¨í•˜ì—¬ ë‚´ë³´ë‚´ê¸°ë¥¼ ì¤‘ë‹¨í•©ë‹ˆë‹¤.")
        return

    # ë‚´ë³´ë‚´ê¸°
    output_dir = Path(args.output_dir)

    if args.format == "all":
        exporter.export_all(str(output_dir))
    elif args.format == "onnx":
        onnx_path = output_dir / "model.onnx"
        exporter.export_to_onnx(str(onnx_path))
    elif args.format == "torchscript":
        ts_path = output_dir / "model_scripted.pt"
        exporter.export_to_torchscript(str(ts_path))
    elif args.format == "quantized":
        quant_path = output_dir / "model_quantized.pt"
        exporter.apply_quantization(str(quant_path))


if __name__ == "__main__":
    main()
