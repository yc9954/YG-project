"""
K-pop ì˜ìƒ ì „ì²˜ë¦¬ ìŠ¤í¬ë¦½íŠ¸
ì˜ìƒì—ì„œ ëª¨ì…˜ ë°ì´í„°ë¥¼ ì¶”ì¶œí•˜ì—¬ HumanML3D í˜•ì‹ìœ¼ë¡œ ë³€í™˜
"""
import cv2
import numpy as np
import json
from pathlib import Path
from typing import List, Dict, Tuple, Optional
import argparse


class VideoMotionExtractor:
    """
    ë¹„ë””ì˜¤ì—ì„œ ëª¨ì…˜ ë°ì´í„°ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤.
    MediaPipe Poseë¥¼ ì‚¬ìš©í•˜ì—¬ í¬ì¦ˆ ì¶”ì •ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.
    """

    def __init__(self):
        """ì´ˆê¸°í™”"""
        self.pose_detector = None
        self._init_pose_detector()

    def _init_pose_detector(self):
        """MediaPipe Pose ì´ˆê¸°í™”"""
        try:
            import mediapipe as mp
            self.mp_pose = mp.solutions.pose
            self.pose_detector = self.mp_pose.Pose(
                static_image_mode=False,
                model_complexity=2,
                smooth_landmarks=True,
                min_detection_confidence=0.5,
                min_tracking_confidence=0.5
            )
            print("âœ… MediaPipe Pose ì´ˆê¸°í™” ì™„ë£Œ")
        except ImportError:
            print("âŒ MediaPipeê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. pip install mediapipeë¥¼ ì‹¤í–‰í•˜ì„¸ìš”.")
            raise

    def extract_motion_from_video(
        self,
        video_path: str,
        output_path: Optional[str] = None,
        fps: int = 30
    ) -> np.ndarray:
        """
        ë¹„ë””ì˜¤ì—ì„œ ëª¨ì…˜ ë°ì´í„° ì¶”ì¶œ

        Args:
            video_path: ë¹„ë””ì˜¤ íŒŒì¼ ê²½ë¡œ
            output_path: ì¶œë ¥ íŒŒì¼ ê²½ë¡œ (ì„ íƒì‚¬í•­)
            fps: íƒ€ê²Ÿ FPS

        Returns:
            np.ndarray: ëª¨ì…˜ ë°ì´í„° [frames, joints, 3 (x, y, z)]
        """
        video_path = Path(video_path)
        if not video_path.exists():
            raise FileNotFoundError(f"ë¹„ë””ì˜¤ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {video_path}")

        print(f"ğŸ“¹ ë¹„ë””ì˜¤ ì²˜ë¦¬ ì¤‘: {video_path}")

        # ë¹„ë””ì˜¤ ë¡œë“œ
        cap = cv2.VideoCapture(str(video_path))
        if not cap.isOpened():
            raise RuntimeError(f"ë¹„ë””ì˜¤ë¥¼ ì—´ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {video_path}")

        # ë¹„ë””ì˜¤ ì •ë³´
        video_fps = int(cap.get(cv2.CAP_PROP_FPS))
        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
        width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
        height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))

        print(f"   ë¹„ë””ì˜¤ ì •ë³´: {width}x{height}, {video_fps}fps, {total_frames}í”„ë ˆì„")

        # í”„ë ˆì„ ìƒ˜í”Œë§ ê°„ê²© ê³„ì‚°
        frame_interval = max(1, int(video_fps / fps))

        motion_data = []
        frame_count = 0
        processed_count = 0

        while cap.isOpened():
            ret, frame = cap.read()
            if not ret:
                break

            # FPS ë§ì¶°ì„œ ìƒ˜í”Œë§
            if frame_count % frame_interval == 0:
                # BGRì„ RGBë¡œ ë³€í™˜
                frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)

                # í¬ì¦ˆ ì¶”ì •
                results = self.pose_detector.process(frame_rgb)

                if results.pose_landmarks:
                    # ëœë“œë§ˆí¬ ì¶”ì¶œ
                    landmarks = self._extract_landmarks(results.pose_landmarks, width, height)
                    motion_data.append(landmarks)
                    processed_count += 1
                else:
                    # ëœë“œë§ˆí¬ê°€ ê°ì§€ë˜ì§€ ì•Šìœ¼ë©´ ì´ì „ í”„ë ˆì„ ë³µì‚¬ ë˜ëŠ” ì œë¡œ
                    if motion_data:
                        motion_data.append(motion_data[-1])
                    else:
                        motion_data.append(np.zeros((33, 3)))

            frame_count += 1

            # ì§„í–‰ ìƒí™© í‘œì‹œ
            if frame_count % 100 == 0:
                print(f"   ì²˜ë¦¬ ì¤‘: {frame_count}/{total_frames} í”„ë ˆì„")

        cap.release()

        if not motion_data:
            raise RuntimeError("í¬ì¦ˆë¥¼ ê°ì§€í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë‹¤ë¥¸ ë¹„ë””ì˜¤ë¥¼ ì‹œë„í•˜ì„¸ìš”.")

        motion_array = np.array(motion_data)
        print(f"âœ… ëª¨ì…˜ ì¶”ì¶œ ì™„ë£Œ: {motion_array.shape[0]}í”„ë ˆì„, {motion_array.shape[1]}ê´€ì ˆ")
        print(f"   í¬ì¦ˆ ê°ì§€ìœ¨: {processed_count}/{len(motion_data)} ({processed_count/len(motion_data)*100:.1f}%)")

        # ì €ì¥
        if output_path:
            output_path = Path(output_path)
            output_path.parent.mkdir(parents=True, exist_ok=True)
            np.save(output_path, motion_array)
            print(f"ğŸ’¾ ëª¨ì…˜ ë°ì´í„° ì €ì¥: {output_path}")

        return motion_array

    def _extract_landmarks(self, pose_landmarks, width: int, height: int) -> np.ndarray:
        """
        MediaPipe ëœë“œë§ˆí¬ë¥¼ numpy ë°°ì—´ë¡œ ë³€í™˜

        Args:
            pose_landmarks: MediaPipe pose landmarks
            width: ì´ë¯¸ì§€ ë„ˆë¹„
            height: ì´ë¯¸ì§€ ë†’ì´

        Returns:
            np.ndarray: [33, 3] í˜•íƒœì˜ ëœë“œë§ˆí¬ ì¢Œí‘œ
        """
        landmarks = np.zeros((33, 3))

        for idx, landmark in enumerate(pose_landmarks.landmark):
            # ì •ê·œí™”ëœ ì¢Œí‘œë¥¼ í”½ì…€ ì¢Œí‘œë¡œ ë³€í™˜
            landmarks[idx, 0] = landmark.x * width
            landmarks[idx, 1] = landmark.y * height
            landmarks[idx, 2] = landmark.z * width  # zë„ ìŠ¤ì¼€ì¼ ì¡°ì •

        return landmarks

    def convert_to_humanml3d(
        self,
        motion_data: np.ndarray,
        fps: int = 20
    ) -> np.ndarray:
        """
        MediaPipe í¬ì¦ˆë¥¼ HumanML3D í˜•ì‹ìœ¼ë¡œ ë³€í™˜

        Args:
            motion_data: MediaPipe ëª¨ì…˜ ë°ì´í„° [frames, 33, 3]
            fps: íƒ€ê²Ÿ FPS (HumanML3DëŠ” 20fps)

        Returns:
            np.ndarray: HumanML3D í˜•ì‹ [frames, joints, features]
        """
        print("ğŸ”„ HumanML3D í˜•ì‹ìœ¼ë¡œ ë³€í™˜ ì¤‘...")

        # MediaPipe 33ê°œ ê´€ì ˆì„ 22ê°œ SMPL ê´€ì ˆë¡œ ë§¤í•‘
        # MediaPipe: 0=nose, 11=left_shoulder, 12=right_shoulder, 13=left_elbow, etc.
        # SMPL/HumanML3D: pelvis, left_hip, right_hip, spine, etc.

        mapping = {
            # ì¤‘ì‹¬ ê´€ì ˆ
            0: [23, 24],  # pelvis <- ì¢Œìš° í™ ì¤‘ê°„
            1: [11, 12],  # spine1 <- ì¢Œìš° ì–´ê¹¨ ì¤‘ê°„
            2: [0],       # spine2 <- ì½”
            3: [0],       # spine3 <- ì½”

            # ì™¼ìª½ ë‹¤ë¦¬
            4: [23],      # left_hip
            5: [25],      # left_knee
            6: [27],      # left_ankle
            7: [31],      # left_foot

            # ì˜¤ë¥¸ìª½ ë‹¤ë¦¬
            8: [24],      # right_hip
            9: [26],      # right_knee
            10: [28],     # right_ankle
            11: [32],     # right_foot

            # ì™¼ìª½ íŒ”
            12: [11],     # left_collar
            13: [13],     # left_shoulder -> left_elbow
            14: [15],     # left_elbow -> left_wrist
            15: [19],     # left_wrist -> left_hand

            # ì˜¤ë¥¸ìª½ íŒ”
            16: [12],     # right_collar
            17: [14],     # right_shoulder -> right_elbow
            18: [16],     # right_elbow -> right_wrist
            19: [20],     # right_wrist -> right_hand

            # ë¨¸ë¦¬
            20: [0],      # neck <- ì½”
            21: [0],      # head <- ì½”
        }

        frames = motion_data.shape[0]
        humanml_data = np.zeros((frames, 22, 3))

        for frame_idx in range(frames):
            for smpl_idx, mp_indices in mapping.items():
                # MediaPipe ê´€ì ˆë“¤ì˜ í‰ê·  ìœ„ì¹˜ ì‚¬ìš©
                positions = [motion_data[frame_idx, mp_idx] for mp_idx in mp_indices]
                humanml_data[frame_idx, smpl_idx] = np.mean(positions, axis=0)

        # ì •ê·œí™”: ì¤‘ì‹¬ì„ ì›ì ìœ¼ë¡œ, ìŠ¤ì¼€ì¼ ì¡°ì •
        humanml_data = self._normalize_motion(humanml_data)

        print(f"âœ… ë³€í™˜ ì™„ë£Œ: {humanml_data.shape}")
        return humanml_data

    def _normalize_motion(self, motion_data: np.ndarray) -> np.ndarray:
        """
        ëª¨ì…˜ ë°ì´í„° ì •ê·œí™”

        Args:
            motion_data: [frames, joints, 3]

        Returns:
            np.ndarray: ì •ê·œí™”ëœ ëª¨ì…˜ ë°ì´í„°
        """
        # ê° í”„ë ˆì„ì—ì„œ pelvis(0ë²ˆ ê´€ì ˆ)ë¥¼ ì¤‘ì‹¬ìœ¼ë¡œ ì´ë™
        pelvis = motion_data[:, 0:1, :]
        motion_data = motion_data - pelvis

        # ì „ì²´ ëª¨ì…˜ì˜ ìŠ¤ì¼€ì¼ ì •ê·œí™” (í‰ê·  ê±°ë¦¬ ê¸°ì¤€)
        distances = np.linalg.norm(motion_data, axis=2)
        mean_distance = np.mean(distances[distances > 0])
        if mean_distance > 0:
            motion_data = motion_data / mean_distance

        return motion_data


class MotionDatasetBuilder:
    """
    íŒŒì¸íŠœë‹ì„ ìœ„í•œ ë°ì´í„°ì…‹ ìƒì„±
    """

    def __init__(self, output_dir: str = "training_data"):
        """
        Args:
            output_dir: ë°ì´í„°ì…‹ ì¶œë ¥ ë””ë ‰í† ë¦¬
        """
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)
        self.dataset = []

    def add_sample(
        self,
        motion_data: np.ndarray,
        caption: str,
        video_name: str
    ):
        """
        ë°ì´í„°ì…‹ì— ìƒ˜í”Œ ì¶”ê°€

        Args:
            motion_data: ëª¨ì…˜ ë°ì´í„° [frames, joints, 3]
            caption: í…ìŠ¤íŠ¸ ì„¤ëª…
            video_name: ë¹„ë””ì˜¤ ì´ë¦„
        """
        sample_id = len(self.dataset)
        sample_name = f"sample_{sample_id:04d}"

        # ëª¨ì…˜ ë°ì´í„° ì €ì¥
        motion_path = self.output_dir / f"{sample_name}.npy"
        np.save(motion_path, motion_data)

        # ë©”íƒ€ë°ì´í„° ì¶”ê°€
        self.dataset.append({
            'id': sample_id,
            'name': sample_name,
            'caption': caption,
            'video_name': video_name,
            'frames': motion_data.shape[0],
            'motion_path': str(motion_path)
        })

        print(f"âœ… ìƒ˜í”Œ ì¶”ê°€: {sample_name} - {caption}")

    def save_dataset(self):
        """ë°ì´í„°ì…‹ ë©”íƒ€ë°ì´í„° ì €ì¥"""
        metadata_path = self.output_dir / "dataset.json"
        with open(metadata_path, 'w', encoding='utf-8') as f:
            json.dump(self.dataset, f, indent=2, ensure_ascii=False)

        print(f"ğŸ’¾ ë°ì´í„°ì…‹ ì €ì¥ ì™„ë£Œ: {metadata_path}")
        print(f"   ì´ ìƒ˜í”Œ ìˆ˜: {len(self.dataset)}")

        return str(metadata_path)


def main():
    parser = argparse.ArgumentParser(description="K-pop ì˜ìƒ ì „ì²˜ë¦¬")
    parser.add_argument("--video", type=str, required=True, help="ë¹„ë””ì˜¤ íŒŒì¼ ê²½ë¡œ")
    parser.add_argument("--caption", type=str, required=True, help="í…ìŠ¤íŠ¸ ì„¤ëª…")
    parser.add_argument("--output-dir", type=str, default="training_data", help="ì¶œë ¥ ë””ë ‰í† ë¦¬")
    parser.add_argument("--fps", type=int, default=20, help="íƒ€ê²Ÿ FPS")

    args = parser.parse_args()

    # ëª¨ì…˜ ì¶”ì¶œ
    extractor = VideoMotionExtractor()
    motion_data = extractor.extract_motion_from_video(args.video, fps=args.fps)

    # HumanML3D í˜•ì‹ ë³€í™˜
    humanml_data = extractor.convert_to_humanml3d(motion_data, fps=args.fps)

    # ë°ì´í„°ì…‹ ìƒì„±
    builder = MotionDatasetBuilder(args.output_dir)
    builder.add_sample(
        motion_data=humanml_data,
        caption=args.caption,
        video_name=Path(args.video).name
    )
    builder.save_dataset()

    print("\nâœ… ì „ì²˜ë¦¬ ì™„ë£Œ!")


if __name__ == "__main__":
    main()
