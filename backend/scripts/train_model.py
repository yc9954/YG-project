"""
K-pop ì•ˆë¬´ ë°ì´í„°ì…‹ìœ¼ë¡œ MDM ëª¨ë¸ íŒŒì¸íŠœë‹
"""
import os
import sys
import json
import torch
import numpy as np
from pathlib import Path
from typing import Dict, List, Optional

# MDM ì €ì¥ì†Œ ê²½ë¡œ ì¶”ê°€
base_dir = Path(__file__).parent.parent
mdm_repo_path = base_dir / "external" / "motion-diffusion-model"

if mdm_repo_path.exists():
    sys.path.insert(0, str(mdm_repo_path))

try:
    from utils.fixseed import fixseed
    from utils.model_util import create_model_and_diffusion
    from utils import dist_util
    from data_loaders.get_data import get_dataset_loader
    MDM_AVAILABLE = True
except ImportError as e:
    print(f"âŒ MDM ëª¨ë“ˆì„ ì„í¬íŠ¸í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {e}")
    print("   ë¨¼ì € setup_ai_model.shë¥¼ ì‹¤í–‰í•˜ì—¬ MDMì„ ì„¤ì •í•˜ì„¸ìš”.")
    MDM_AVAILABLE = False
    sys.exit(1)


class KPopModelTrainer:
    """
    K-pop ì•ˆë¬´ ë°ì´í„°ì…‹ìœ¼ë¡œ MDM ëª¨ë¸ì„ íŒŒì¸íŠœë‹í•©ë‹ˆë‹¤.
    """
    
    def __init__(
        self,
        base_model_path: str,
        training_data_path: str,
        output_dir: str,
        epochs: int = 100,
        batch_size: int = 32,
        learning_rate: float = 1e-4
    ):
        """
        Args:
            base_model_path: ì‚¬ì „ í•™ìŠµëœ MDM ëª¨ë¸ ê²½ë¡œ
            training_data_path: í•™ìŠµ ë°ì´í„° ê²½ë¡œ (JSON ë˜ëŠ” NPZ í˜•ì‹)
            output_dir: í•™ìŠµëœ ëª¨ë¸ ì €ì¥ ë””ë ‰í† ë¦¬
            epochs: í•™ìŠµ ì—í¬í¬ ìˆ˜
            batch_size: ë°°ì¹˜ í¬ê¸°
            learning_rate: í•™ìŠµë¥ 
        """
        self.base_model_path = base_model_path
        self.training_data_path = training_data_path
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        self.epochs = epochs
        self.batch_size = batch_size
        self.learning_rate = learning_rate
        
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        dist_util.setup_dist(device=0 if torch.cuda.is_available() else -1)
        
        self.model = None
        self.diffusion = None
        self.optimizer = None
    
    def load_base_model(self):
        """ì‚¬ì „ í•™ìŠµëœ ëª¨ë¸ ë¡œë“œ"""
        print(f"ğŸ“¥ ì‚¬ì „ í•™ìŠµ ëª¨ë¸ ë¡œë“œ ì¤‘: {self.base_model_path}")
        
        # args.json ë¡œë“œ
        args_path = Path(self.base_model_path).parent / "args.json"
        with open(args_path, 'r') as f:
            args_dict = json.load(f)
        
        from argparse import Namespace
        args = Namespace(**args_dict)
        
        # ë°ì´í„° ë¡œë” ìƒì„±
        original_cwd = os.getcwd()
        try:
            os.chdir(str(mdm_repo_path))
            data = get_dataset_loader(
                name=args.dataset,
                batch_size=self.batch_size,
                num_frames=196,
                split='train',
                hml_mode='text_only'
            )
        finally:
            os.chdir(original_cwd)
        
        # ëª¨ë¸ ë° Diffusion ìƒì„±
        self.model, self.diffusion = create_model_and_diffusion(args, data)
        
        # ì‚¬ì „ í•™ìŠµëœ ê°€ì¤‘ì¹˜ ë¡œë“œ
        checkpoint = torch.load(self.base_model_path, map_location=self.device)
        self.model.load_state_dict(checkpoint)
        self.model.to(self.device)
        
        # ì˜µí‹°ë§ˆì´ì € ì„¤ì •
        self.optimizer = torch.optim.Adam(
            self.model.parameters(),
            lr=self.learning_rate
        )
        
        print("âœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
    
    def load_training_data(self) -> List[Dict]:
        """
        í•™ìŠµ ë°ì´í„° ë¡œë“œ
        ë°ì´í„° í˜•ì‹: [{"motion": np.ndarray, "text": str, "style": str}, ...]
        """
        print(f"ğŸ“¥ í•™ìŠµ ë°ì´í„° ë¡œë“œ ì¤‘: {self.training_data_path}")
        
        training_data_path = Path(self.training_data_path)
        
        if training_data_path.suffix == '.json':
            with open(training_data_path, 'r') as f:
                data = json.load(f)
        elif training_data_path.suffix == '.npz':
            data = np.load(training_data_path, allow_pickle=True)
            data = data['data'].tolist()
        else:
            raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ë°ì´í„° í˜•ì‹: {training_data_path.suffix}")
        
        print(f"âœ… í•™ìŠµ ë°ì´í„° ë¡œë“œ ì™„ë£Œ: {len(data)}ê°œ ìƒ˜í”Œ")
        return data
    
    def train(self):
        """ëª¨ë¸ í•™ìŠµ"""
        if self.model is None:
            raise RuntimeError("ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. load_base_model()ì„ ë¨¼ì € í˜¸ì¶œí•˜ì„¸ìš”.")
        
        # í•™ìŠµ ë°ì´í„° ë¡œë“œ
        training_data = self.load_training_data()
        
        print(f"ğŸš€ í•™ìŠµ ì‹œì‘ (ì—í¬í¬: {self.epochs}, ë°°ì¹˜ í¬ê¸°: {self.batch_size})")
        
        for epoch in range(self.epochs):
            total_loss = 0.0
            num_batches = 0
            
            # ë°°ì¹˜ ìƒì„± ë° í•™ìŠµ
            for i in range(0, len(training_data), self.batch_size):
                batch = training_data[i:i + self.batch_size]
                
                # ë°°ì¹˜ ë°ì´í„° ì¤€ë¹„
                motions = torch.stack([
                    torch.from_numpy(item['motion']).float()
                    for item in batch
                ]).to(self.device)
                
                texts = [item['text'] for item in batch]
                
                # ëª¨ë¸ forward
                self.optimizer.zero_grad()
                
                # Diffusion loss ê³„ì‚°
                t = torch.randint(
                    0, self.diffusion.num_timesteps,
                    (motions.shape[0],), device=self.device
                )
                noise = torch.randn_like(motions)
                x_t = self.diffusion.q_sample(motions, t, noise=noise)
                
                # í…ìŠ¤íŠ¸ ì„ë² ë”©
                model_kwargs = {'y': {'text': texts}}
                if hasattr(self.model, 'encode_text'):
                    model_kwargs['y']['text_embed'] = self.model.encode_text(texts)
                
                # ì˜ˆì¸¡
                model_output = self.model(x_t, t, **model_kwargs)
                
                # Loss ê³„ì‚°
                loss = torch.nn.functional.mse_loss(model_output, noise)
                
                # Backward
                loss.backward()
                self.optimizer.step()
                
                total_loss += loss.item()
                num_batches += 1
            
            avg_loss = total_loss / num_batches if num_batches > 0 else 0.0
            print(f"Epoch {epoch + 1}/{self.epochs}, Loss: {avg_loss:.6f}")
            
            # ì²´í¬í¬ì¸íŠ¸ ì €ì¥ (ë§¤ 10 ì—í¬í¬ë§ˆë‹¤)
            if (epoch + 1) % 10 == 0:
                checkpoint_path = self.output_dir / f"checkpoint_epoch_{epoch + 1}.pt"
                torch.save(self.model.state_dict(), checkpoint_path)
                print(f"ğŸ’¾ ì²´í¬í¬ì¸íŠ¸ ì €ì¥: {checkpoint_path}")
        
        # ìµœì¢… ëª¨ë¸ ì €ì¥
        final_model_path = self.output_dir / "final_model.pt"
        torch.save(self.model.state_dict(), final_model_path)
        print(f"âœ… í•™ìŠµ ì™„ë£Œ! ìµœì¢… ëª¨ë¸ ì €ì¥: {final_model_path}")


def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    import argparse
    
    parser = argparse.ArgumentParser(description="K-pop ì•ˆë¬´ ë°ì´í„°ì…‹ìœ¼ë¡œ MDM ëª¨ë¸ íŒŒì¸íŠœë‹")
    parser.add_argument("--base_model", type=str, required=True,
                       help="ì‚¬ì „ í•™ìŠµëœ MDM ëª¨ë¸ ê²½ë¡œ")
    parser.add_argument("--data", type=str, required=True,
                       help="í•™ìŠµ ë°ì´í„° ê²½ë¡œ (JSON ë˜ëŠ” NPZ)")
    parser.add_argument("--output", type=str, required=True,
                       help="ì¶œë ¥ ë””ë ‰í† ë¦¬")
    parser.add_argument("--epochs", type=int, default=100,
                       help="í•™ìŠµ ì—í¬í¬ ìˆ˜ (ê¸°ë³¸ê°’: 100)")
    parser.add_argument("--batch_size", type=int, default=32,
                       help="ë°°ì¹˜ í¬ê¸° (ê¸°ë³¸ê°’: 32)")
    parser.add_argument("--lr", type=float, default=1e-4,
                       help="í•™ìŠµë¥  (ê¸°ë³¸ê°’: 1e-4)")
    
    args = parser.parse_args()
    
    trainer = KPopModelTrainer(
        base_model_path=args.base_model,
        training_data_path=args.data,
        output_dir=args.output,
        epochs=args.epochs,
        batch_size=args.batch_size,
        learning_rate=args.lr
    )
    
    trainer.load_base_model()
    trainer.train()


if __name__ == "__main__":
    main()

